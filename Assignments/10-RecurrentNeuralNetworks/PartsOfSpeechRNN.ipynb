{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d08c513",
   "metadata": {},
   "source": [
    "# Predicting Parts-of-Speech with an LSTM\n",
    "\n",
    "Let's preview the end result. We want to take a sentence and output the part-of-speech for each word in that sentence. Something like this:\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "new_sentence = \"I is a teeth\"\n",
    "\n",
    "... # Preprocessing the sentence\n",
    "\n",
    "# Acting on the preprocessed sentence\n",
    "predictions = model(word_indices)\n",
    "\n",
    "... # Formatting the output\n",
    "```\n",
    "\n",
    "**Output**\n",
    "\n",
    "```text\n",
    "I     => Noun\n",
    "is    => Verb\n",
    "a     => Determiner\n",
    "teeth => Noun\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50228bb",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. Add two additional sentences (for every member of your group) to the [list on this google sheet](https://docs.google.com/spreadsheets/d/1HJmlehaYhGWclDo1t0k6i1VHxN15zr8ZmJj7Rf_VEaI/edit#gid=1031300490). You can thank previous semesters for the existing dataset below.\n",
    "\n",
    "1. **Do not run all cells in the notebook.** You will need to make some predictions prior to running cells. Read through the notebook as a group, stopping and answering each question on gradescope as you go.\n",
    "\n",
    "1. After you work through the notebook once, you should try to improve accuracy by changing hyperparameters (including the network parameters and network architecture--extending the classification part of the network might be a good idea).\n",
    "\n",
    "1. (Optional) Try changing the model out for a\n",
    "    + fully connected network,\n",
    "    + convolutional neural network, or\n",
    "    + transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdad3bb",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484b5fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "from fastprogress.fastprogress import progress_bar, master_bar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "jtplot.style(context=\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e919c0d",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d3e4e0",
   "metadata": {},
   "source": [
    "### Raw dataset\n",
    "\n",
    "- Add two sentences and corresponding parts of speech **per group member**\n",
    "- You can use this utility for double checking your parts of speech: https://parts-of-speech.info/\n",
    "- I will put them into the notebook (you will need to pull the updates)\n",
    "- Do not include any punctuation\n",
    "- Your sentences must only include nouns, verbs, and determiners\n",
    "    + N for noun\n",
    "    + V for verb\n",
    "    + D for determiner\n",
    "- You can mark pronouns as nouns\n",
    "\n",
    "You can find an exmaples in the [spreadsheet](https://docs.google.com/spreadsheets/d/1HJmlehaYhGWclDo1t0k6i1VHxN15zr8ZmJj7Rf_VEaI/edit#gid=1031300490)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will update this once everyone has submitted their sentences in the sheet.\n",
    "# You will then need to pull the latest version of this notebook.\n",
    "\n",
    "raw_dataset = [\n",
    "    (\"The dog ate the apple\", \"D N V D N\"),\n",
    "    (\"Everybody read that book\", \"N V D N\"),\n",
    "    (\"Trapp is sleeping\", \"N V V\"),\n",
    "    (\"Everybody ate the apple\", \"N V D N\"),\n",
    "    (\"Cats are good\", \"N V D\"),\n",
    "    (\"Dogs are not as good as cats\", \"N V D D D D N\"),\n",
    "    (\"Dogs eat dog food\", \"N V N N\"),\n",
    "    (\"Watermelon is the best food\", \"N V D D N\"),\n",
    "    (\"I want a milkshake right now\", \"N V D N D D\"),\n",
    "    (\"I have too much homework\", \"N V D D N\"),\n",
    "    (\"Zoom won't work\", \"N D V\"),\n",
    "    (\"Pie also sounds good\", \"N D V D\"),\n",
    "    (\"The college is having the department fair this Friday\", \"D N V V D N N D N\"),\n",
    "    (\"Research interests span many areas\", \"N N V D N\"),\n",
    "    (\"Alex is finishing his Ph.D\", \"N V V D N\"),\n",
    "    (\"She is the author\", \"N V D N\"),\n",
    "    (\"It is almost the end of the semester\", \"N V D D N D D N\"),\n",
    "    (\"Blue is a color\", \"N V D N\"),\n",
    "    (\"They wrote a book\", \"N V D N\"),\n",
    "    (\"The syrup covers the pancake\", \"D N V D N\"),\n",
    "    (\"Harrison has these teeth\", \"N V D N\"),\n",
    "    (\"The numbers are fractions\", \"D N V N\"),\n",
    "    (\"Yesterday happened\", \"N V\"),\n",
    "    (\"Caramel is sweet\", \"N V D\"),\n",
    "    (\"Computers use electricity\", \"N V N\"),\n",
    "    (\"Gold is a valuable thing\", \"N V D D N\"),\n",
    "    (\"This extension cord helps\", \"D D N V\"),\n",
    "    (\"It works on my machine\", \"N V D D N\"),\n",
    "    (\"We have the words\", \"N V D N\"),\n",
    "    (\"Trapp is a dog\", \"N V D N\"),\n",
    "    (\"This is a computer\", \"N V D N\"),\n",
    "    (\"I love lamps\", \"N V N\"),\n",
    "    (\"I walked outside\", \"N V N\"),\n",
    "    (\"You never bike home\", \"N D V N\"),\n",
    "    (\"You are a wizard Harry\", \"N V D N N\"),\n",
    "    (\"Trapp ate the shoe\", \"N V D N\"),\n",
    "    (\"Jett failed his test\", \"N V D N\"),\n",
    "    (\"Alice won the game\", \"N V D N\"),\n",
    "    (\"The class lasted a semester\", \"D N V D N\"),\n",
    "    (\"The tree had a branch\", \"D N V D N\"),\n",
    "    (\"I ran a race\", \"N V D N\"),\n",
    "    (\"The dog barked\", \"D N V\"),\n",
    "    (\"Toby hit the wall\", \"N V D N\"),\n",
    "    (\"Zayn ate an apple\", \"N V D N\"),\n",
    "    (\"The cat fought the dog\", \"D N V D N\"),\n",
    "    (\"I got an A\", \"N V D N\"),\n",
    "    (\"The A hurt\", \"D N V\"),\n",
    "    (\"I jump\", \"N V\"),\n",
    "    (\"I drank a yerb\", \"N V D N\"),\n",
    "    (\"The snake ate a fruit\", \"D N V D N\"),\n",
    "    (\"I played the game\", \"N V D N\"),\n",
    "    (\"I watched a movie\", \"N V D N\"),\n",
    "    (\"Clark fixed the audio\", \"N V D N\"),\n",
    "    (\"I went to Frary\", \"N V D N\"),\n",
    "    (\"I go to Pomona\", \"N V D N\"),\n",
    "    (\"Food are friends not fish\", \"N V N D N\"),\n",
    "    (\"You are reading this\", \"N V D N\"),\n",
    "    (\"Wonderland protocol is amazing\", \"D N V D\"),\n",
    "    (\"This is a sentence\", \"D V D N\"),\n",
    "    (\"I should be doing homework\", \"N V V V N\"),\n",
    "    (\"Computers are tools\", \"N V N\"),\n",
    "    (\"The whale swims\", \"D N V\"),\n",
    "    (\"A cup is filled\", \"D N V V\"),\n",
    "    (\"This is a cat\", \"D V D N\"),\n",
    "    (\"These are trees\", \"D V N\"),\n",
    "    (\"The cat is the teacher\", \"D N V D N\"),\n",
    "    (\"I ate food today\", \"N V N N\"),\n",
    "    (\"I am a human\", \"N V D N\"),\n",
    "    (\"The cat sleeps\", \"D N V\"),\n",
    "    (\"Whales are mammals\", \"N V N\"),\n",
    "    (\"I like turtles\", \"N V N\"),\n",
    "    (\"A shark ate me\", \"D N V N\"),\n",
    "    (\"There are mirrors\", \"D V N\"),\n",
    "    (\"The bus spins\", \"D N V\"),\n",
    "    (\"Computers are machines\", \"N V N\"),\n",
    "    (\"Beckett is a dancer\", \"N V D N\"),\n",
    "    (\"Networks are things\", \"N V N\"),\n",
    "    (\"The lady killed a cat\", \"D N V D N\"),\n",
    "    (\"Summer is tomorrow\", \"N V N\"),\n",
    "    (\"A girl cries\", \"D N V\"),\n",
    "    (\"I am a dog\", \"N V D N\"),\n",
    "    (\"Orange is the fruit\", \"N V D N\"),\n",
    "    (\"Mary had a lamb\", \"N V D N\"),\n",
    "    (\"She died yesterday\", \"N V N\"),\n",
    "    (\"The dog jumped\", \"D N V\"),\n",
    "    (\"The man ran\", \"D N V\"),\n",
    "    (\"The sun slept\", \"D N V\"),\n",
    "    (\"the computer is dying\", \"D N V V\"),\n",
    "    (\"Alan likes pears\", \"N V N\"),\n",
    "    (\"I am the octopus\", \"N V D N\"),\n",
    "    (\"This is a sentence\", \"D V D N\"),\n",
    "    (\"The dog walked\", \"D N V\"),\n",
    "    (\"The wind was blowing yesterday\", \"D N V V N\"),\n",
    "    (\"The laptop cried\", \"D N V\"),\n",
    "    (\"I like running\", \"N V V \"),\n",
    "    (\"He hates cats\", \"N V N\"),\n",
    "    (\"Alan wants food\", \"N V N\"),\n",
    "    (\"It is a baby\", \"N V D N\"),\n",
    "    (\"I had a donut\", \"N V D N\"),\n",
    "    (\"Blotto is game\", \"N V N\"),\n",
    "    (\"Game math win\", \"N N N\"),\n",
    "    (\"Nutella is a topping\", \"N V D N\"),\n",
    "    (\"Work takes time\", \"N V N\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c619a9cf",
   "metadata": {},
   "source": [
    "### Preprocess raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e9c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence):\n",
    "    \"\"\"Convert a string into a list of lowercased words.\"\"\"\n",
    "    return sentence.lower().split()\n",
    "\n",
    "\n",
    "def process_parts(parts):\n",
    "    \"\"\"Break the parts into individual list elements.\"\"\"\n",
    "    return parts.split()\n",
    "\n",
    "\n",
    "dataset = [(process_sentence(s), process_parts(p)) for s, p in raw_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855326b",
   "metadata": {},
   "source": [
    "### Prepare data for use as NN input\n",
    "\n",
    "We can't pass a list of plain text words and parts-of-speech to a NN. We need to convert them to a more appropriate format.\n",
    "\n",
    "We'll start by creating a unique index for each word and part-of-speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e114fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all unique words\n",
    "word_to_index = {}\n",
    "word_counts = {}\n",
    "total_words = 0\n",
    "\n",
    "# Grab all unique parts-of-speech\n",
    "part_to_index = {}\n",
    "part_counts = {}\n",
    "part_list = []\n",
    "total_parts = 0\n",
    "\n",
    "for words, parts in dataset:\n",
    "\n",
    "    # Need a part-of-speech for every word\n",
    "    assert len(words) == len(parts)\n",
    "\n",
    "    # Process words\n",
    "    total_words += len(words)\n",
    "\n",
    "    for word in words:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            word_counts[word] = 0\n",
    "        word_counts[word] += 1\n",
    "\n",
    "    # Process parts\n",
    "    total_parts += len(parts)\n",
    "\n",
    "    for part in parts:\n",
    "        if part not in part_to_index:\n",
    "            part_to_index[part] = len(part_to_index)\n",
    "            part_counts[part] = 0\n",
    "            part_list.append(part)\n",
    "        part_counts[part] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a91d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of words:\", total_words)\n",
    "print(\"Number of unique words:\", len(word_to_index))\n",
    "\n",
    "print()\n",
    "print(\"       Vocabulary Indices\")\n",
    "print(\"--------------------------------\")\n",
    "print(\"          Word => Index (Count)\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "for word in sorted(word_to_index):\n",
    "    print(f\"{word:>14} => {word_to_index[word]:>3} ({word_counts[word]:>2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of parts-of-speech:\", total_parts)\n",
    "print(\"Number of unique parts-of-speech:\", len(part_to_index))\n",
    "\n",
    "print()\n",
    "print(\" Part Indices\")\n",
    "print(\"--------------\")\n",
    "\n",
    "for part, index in part_to_index.items():\n",
    "    print(f\" {part} => {index} ({part_counts[part]:>3} / {total_parts} = {100*part_counts[part]/total_parts:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb79e3",
   "metadata": {},
   "source": [
    "### Question: What is the highest accuracy you'd expect from a \"dumb\" classifier (hint: look at the distribution of the targets in the output above)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d142bdca",
   "metadata": {},
   "source": [
    "## Building a Parts-of-Speech Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833357a4",
   "metadata": {},
   "source": [
    "### Word embeddings\n",
    "\n",
    "Once we have a unique identifier for each word, it is useful to start our NN with an [embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding) layer. This layer converts an index into a vector of values.\n",
    "\n",
    "You can think of each value as indicating something about the word. For example, maybe the first value indicates how much a word conveys happiness vs sadness. Of course, the NN can learn any attributes and it is not limited to thinks like happy/sad, masculine/feminine, etc.\n",
    "\n",
    "This is an important concept in natual language processing. It enables the network to consider two distinct words as *similar*---synonyms would share similar embedding values.\n",
    "\n",
    "**Creating an embedding layer**. An embedding layer is created by telling it the size of the vocabulary (the number of words) and an embedding dimension (how many values to use to represent a word).\n",
    "\n",
    "**Embedding layer input and output**. An embedding layer takes a word index and return a corresponding embedding as a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dd9811",
   "metadata": {},
   "source": [
    "#### Question: What do you expect to see printed for the indices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a695b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_indices(words, mapping):\n",
    "    \"\"\"Convert a word (like \"apple\") into an index (like 4).\"\"\"\n",
    "    indices = [mapping[w] for w in words]\n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "\n",
    "words = [\"trapp\", \"computer\"]\n",
    "print(\"An example mapping of words to indices.\")\n",
    "print(\"Words:\", words)\n",
    "print(\"Indices:\", to_indices(words, word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vocab size is determined by how many words you expect to train on\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "# We get to pick the number of parameters that represent a word\n",
    "embed_dim = 6\n",
    "\n",
    "embed_layer = torch.nn.Embedding(vocab_size, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab86bce6",
   "metadata": {},
   "source": [
    "#### Question: What is the expected shape of `embed_output`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24739098",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The dog ate the apple\"\n",
    "words = process_sentence(sentence)\n",
    "indices = to_indices(words, word_to_index)\n",
    "\n",
    "# Test out our untrained embedding layer\n",
    "embed_output = embed_layer(indices)\n",
    "print(\"Indices shape:\", indices.shape)\n",
    "print(\"Indices values:\", indices)\n",
    "print(\"Embedding output shape:\", embed_output.shape)\n",
    "print(f\"Embedding values:\\n{embed_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e167a05",
   "metadata": {},
   "source": [
    "### Adding an LSTM (RNN) layer\n",
    "\n",
    "The [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) layer is in charge of processing embeddings such that the network can output the correct classification. Since this is a recurrent layer, it will take into account past words when it creates an output for the current word.\n",
    "\n",
    "**Creating an LSTM layer**. To create an LSTM you need to tell it the size of its input (the size of an embedding) and the size of its internal cell state.\n",
    "\n",
    "**LSTM layer input and output**. An LSTM takes an embedding (and optionally an initial hidden and cell state) and outputs a value for each word as well as the current hidden and cell state).\n",
    "\n",
    "If you read the linked LSTM documentation you will see that it requires input in this format: `(seq_len, batch, input_size)`.\n",
    "\n",
    "As you can see above, our embedding layer outputs something that is `(seq_len, input_size)`. So, we need to add a dimension in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 10  # Hyperparameter\n",
    "num_layers = 5  # Hyperparameter\n",
    "\n",
    "lstm_layer = torch.nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0337e8",
   "metadata": {},
   "source": [
    "#### Question: What is the expected shape of `lstm_output`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486bb0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LSTM layer expects the input to be in the shape (L, N, E)\n",
    "#   L is the length of the sequence\n",
    "#   N is the batch size (we'll stick with 1 here)\n",
    "#   E is the size of the embedding\n",
    "\n",
    "# Add the N dimension\n",
    "lstm_input = embed_output.unsqueeze(1)\n",
    "\n",
    "# We can ignore the second output of the lstm_layer for now\n",
    "lstm_output, _ = lstm_layer(lstm_input)\n",
    "\n",
    "print(\"LSTM output shape:\", lstm_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e655160d",
   "metadata": {},
   "source": [
    "### Classifiying the LSTM output\n",
    "\n",
    "We can now add a fully connected, [linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) layer to our NN to classify the word's part-of-speech.\n",
    "\n",
    "**Creating a linear layer**. We create a linear layer by specifying the shape of the input into the layer and the number of neurons in the linear layer.\n",
    "\n",
    "**Linear layer input and output**. The input is expected to be `(input_size, output_size)` and the output will be the output of each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75440441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out network needs an output for each possible part-of-speech\n",
    "parts_size = len(part_to_index)\n",
    "\n",
    "linear_layer = torch.nn.Linear(hidden_dim, parts_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc28453",
   "metadata": {},
   "source": [
    "#### Question: What is the expected shape of `linear_output`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefce339",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_output = linear_layer(lstm_output)\n",
    "\n",
    "print(\"Linear output shape:\", linear_output.shape)\n",
    "print(f\"Linear output:\\n{linear_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566eb1a2",
   "metadata": {},
   "source": [
    "## Training an LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed534cbe",
   "metadata": {},
   "source": [
    "### Setting all hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/validation split\n",
    "valid_percent = 0.15\n",
    "\n",
    "# Size of word embedding\n",
    "embed_dim = 8\n",
    "\n",
    "# Size of LSTM internal state\n",
    "hidden_dim = 8\n",
    "\n",
    "# Number of LSTM layers\n",
    "num_layers = 1\n",
    "\n",
    "# Optimization hyperparameters\n",
    "learning_rate = 0.005\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27273ef",
   "metadata": {},
   "source": [
    "### Splitting the dataset into training and validation partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f53886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dataset)\n",
    "vocab_size = len(word_to_index)  # Number of unique input words\n",
    "parts_size = len(part_to_index)  # Number of unique output targets\n",
    "\n",
    "# Shuffle the data so that we can split the dataset randomly\n",
    "shuffle(dataset)\n",
    "\n",
    "split_point = int(N * valid_percent)\n",
    "valid_dataset = dataset[:split_point]\n",
    "train_dataset = dataset[split_point:]\n",
    "\n",
    "print(\"Size of validation dataset:\", len(train_dataset))\n",
    "print(\"Size of validation dataset:\", len(valid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69bd9c",
   "metadata": {},
   "source": [
    "### Creating the parts-of-speech LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3293433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POS_LSTM(torch.nn.Module):\n",
    "    \"\"\"Parts-of-speech LSTM model.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, parts_size):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = torch.nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, parts_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embed(X)\n",
    "        X, _ = self.lstm(X.unsqueeze(1))\n",
    "        return self.linear(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ee5bc3",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c09deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(mb, dataset, model, criterion, optimizer):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for words, parts in progress_bar(dataset, parent=mb):\n",
    "\n",
    "        mb.child.comment = \"Training\"\n",
    "        \n",
    "        word_indices = to_indices(words, word_to_index)\n",
    "        part_indices = to_indices(parts, part_to_index)\n",
    "\n",
    "        part_scores = model(word_indices)\n",
    "\n",
    "        loss = criterion(part_scores.squeeze(), part_indices)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataset)\n",
    "\n",
    "\n",
    "def validate(mb, dataset, model, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_words = 0\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        data_iter = progress_bar(dataset, parent=mb) if mb else iter(dataset)\n",
    "        for words, parts in data_iter:\n",
    "\n",
    "            if mb:\n",
    "                mb.child.comment = f\"Validation\"\n",
    "\n",
    "            total_words += len(words)\n",
    "\n",
    "            word_indices = to_indices(words, word_to_index)\n",
    "            part_indices = to_indices(parts, part_to_index)\n",
    "\n",
    "            part_scores = model(word_indices).squeeze()\n",
    "\n",
    "            loss = criterion(part_scores.squeeze(), part_indices)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions = part_scores.argmax(dim=1)\n",
    "            total_correct += sum(t == part_list[p] for t, p in zip(parts, predictions))\n",
    "\n",
    "    return total_correct * 100 / total_words, total_loss / len(dataset)\n",
    "\n",
    "\n",
    "def update_plots(mb, train_losses, valid_losses, epoch, num_epochs):\n",
    "\n",
    "    # Update plot data\n",
    "    max_loss = max(max(train_losses), max(valid_losses))\n",
    "    min_loss = min(min(train_losses), min(valid_losses))\n",
    "\n",
    "    x_margin = 0.2\n",
    "    x_bounds = [0 - x_margin, num_epochs + x_margin]\n",
    "\n",
    "    y_margin = 0.1 * (max_loss - min_loss)\n",
    "    y_bounds = [min_loss - y_margin, max_loss + y_margin]\n",
    "\n",
    "    train_xaxis = torch.linspace(0, epoch + 1, len(train_losses))\n",
    "    valid_xaxis = torch.linspace(0, epoch + 1, len(valid_losses))\n",
    "    graph_data = [[train_xaxis, train_losses], [valid_xaxis, valid_losses]]\n",
    "\n",
    "    mb.update_graph(graph_data, x_bounds, y_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c875a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = POS_LSTM(vocab_size, embed_dim, hidden_dim, num_layers, parts_size)\n",
    "\n",
    "summary(model)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "accuracies = []\n",
    "\n",
    "mb = master_bar(range(num_epochs))\n",
    "mb.names = [\"Train Loss\", \"Valid Loss\"]\n",
    "mb.main_bar.comment = f\"Epochs\"\n",
    "\n",
    "accuracy, valid_loss = validate(None, valid_dataset, model, criterion)\n",
    "valid_losses.append(valid_loss)\n",
    "accuracies.append(accuracy)\n",
    "\n",
    "for epoch in mb:\n",
    "\n",
    "    # Shuffle the data for each epoch (stochastic gradient descent)\n",
    "    shuffle(train_dataset)\n",
    "\n",
    "    train_loss = train_one_epoch(mb, train_dataset, model, criterion, optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    accuracy, valid_loss = validate(mb, valid_dataset, model, criterion)\n",
    "    valid_losses.append(valid_loss)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    update_plots(mb, train_losses, valid_losses, epoch, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e47f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies, \"--o\")\n",
    "plt.title(f\"Accuracy (Final={accuracies[-1]:.2f}%)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "_ = plt.ylim([0, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e780e3",
   "metadata": {},
   "source": [
    "### Examining results\n",
    "\n",
    "Here we look at all words that are misclassified by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc42165",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mis-predictions on entire dataset after training\")\n",
    "header = \"Word\".center(14) + \" | True Part | Prediction\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for words, parts in dataset:\n",
    "        \n",
    "        word_indices = to_indices(words, word_to_index)\n",
    "        \n",
    "        part_scores = model(word_indices)\n",
    "        \n",
    "        predictions = part_scores.squeeze().argmax(dim=1)\n",
    "        \n",
    "        for word, part, pred in zip(words, parts, predictions):\n",
    "            \n",
    "            if part != part_list[pred]:\n",
    "                print(f\"{word:>14} |     {part}     |    {part_list[pred]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf48486",
   "metadata": {},
   "source": [
    "## Using the Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc88aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = \"I is a teeth\"\n",
    "\n",
    "# Convert sentence to lowercase words\n",
    "words = process_sentence(new_sentence)\n",
    "\n",
    "# Check that each word is in our vocabulary\n",
    "for word in words:\n",
    "    assert word in word_to_index\n",
    "\n",
    "# Convert input to a tensor\n",
    "word_indices = to_indices(words, word_to_index)\n",
    "\n",
    "# Compute prediction\n",
    "predictions = model(word_indices)\n",
    "predictions = predictions.squeeze().argmax(dim=1)\n",
    "\n",
    "# Print results\n",
    "for word, part in zip(new_sentence.split(), predictions):\n",
    "    print(word, \"=>\", part_list[part.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d17307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupytext --sync PartsOfSpeechRNN.ipynb"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

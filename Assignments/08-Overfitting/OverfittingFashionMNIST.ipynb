{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006465e7",
   "metadata": {},
   "source": [
    "# FashionMNIST Overfitting\n",
    "\n",
    "This notebook runs better in the browser than it does in VSCode.\n",
    "\n",
    "Things to do if you want a challenge:\n",
    "\n",
    "1. Display/plot samples from the dataset\n",
    "2. Plot accuracies (train and valid)\n",
    "3. Add dropout, batch norm, or other overfitting remedies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd26a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "jtplot.style(context=\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f9112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fmnist_data_loaders(path, subset_size, batch_size, valid_batch_size=0):\n",
    "    # Data specific transforms\n",
    "    data_mean = (0.2860,)\n",
    "    data_std = (0.3530,)\n",
    "    xforms = Compose([ToTensor(), Normalize(data_mean, data_std)])\n",
    "\n",
    "    # Training data loader\n",
    "    train_dataset = FashionMNIST(root=path, train=True, download=True, transform=xforms)\n",
    "\n",
    "    # Create a subset of the entire dataset (TODO: ensure all classes are present)\n",
    "    indices = torch.randperm(len(train_dataset))[:subset_size]\n",
    "    train_subset = Subset(train_dataset, indices)\n",
    "\n",
    "    # Set the batch size to N if batch_size is 0\n",
    "    tbs = len(train_dataset) if batch_size == 0 else batch_size\n",
    "    train_loader = DataLoader(train_subset, batch_size=tbs, shuffle=True)\n",
    "\n",
    "    # Validation data loader\n",
    "    valid_dataset = FashionMNIST(\n",
    "        root=path, train=False, download=True, transform=xforms\n",
    "    )\n",
    "\n",
    "    # Set the batch size to N if batch_size is 0\n",
    "    vbs = len(valid_dataset) if valid_batch_size == 0 else valid_batch_size\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=vbs, shuffle=True)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f86418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # The first \"layer\" just rearranges the Nx28x28 input into Nx784\n",
    "        first_layer = nn.Flatten()\n",
    "\n",
    "        # The hidden layers include:\n",
    "        # 1. a linear component (computing Z) and\n",
    "        # 2. a non-linear comonent (computing A)\n",
    "        hidden_layers = [\n",
    "            nn.Sequential(nn.Linear(nlminus1, nl), nn.ReLU())\n",
    "            for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes)\n",
    "        ]\n",
    "\n",
    "        # The output layer must be Linear without an activation. See:\n",
    "        #   https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = [first_layer] + hidden_layers + [output_layer]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd09b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_N = 8000\n",
    "batch_size = 64\n",
    "\n",
    "# Let's use some shared space for the data (so that we don't have copies\n",
    "# sitting around everywhere)\n",
    "data_path = \"/data/cs152/cache/pytorch/data\"\n",
    "\n",
    "# # Use the GPUs if they are available\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"Using '{device}' device.\")\n",
    "device = \"cpu\"\n",
    "\n",
    "train_loader, valid_loader = get_fmnist_data_loaders(data_path, train_N, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df73b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_neurons = [13, 17]\n",
    "\n",
    "# The input layer size depends on the dataset\n",
    "n0 = train_loader.dataset.dataset.data.shape[1:].numel()\n",
    "\n",
    "# The output layer size depends on the dataset\n",
    "nL = len(train_loader.dataset.dataset.classes)\n",
    "\n",
    "# Preprend the input and append the output layer sizes\n",
    "layer_sizes = [n0] + hidden_layer_neurons + [nL]\n",
    "model = NeuralNetwork(layer_sizes).to(device)\n",
    "\n",
    "summary(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c904bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0\n",
    "\n",
    "model = NeuralNetwork(layer_sizes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Information for plots\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# A master bar for fancy output progress\n",
    "mb = master_bar(range(num_epochs))\n",
    "mb.names = [\"Train Loss\", \"Valid Loss\"]\n",
    "\n",
    "for epoch in mb:\n",
    "\n",
    "    #\n",
    "    # Training\n",
    "    #\n",
    "    model.train()\n",
    "\n",
    "    train_N = len(train_loader.dataset)\n",
    "    num_train_batches = len(train_loader)\n",
    "    train_dataiterator = iter(train_loader)\n",
    "\n",
    "    train_loss_mean = 0\n",
    "\n",
    "    for batch in progress_bar(range(num_train_batches), parent=mb):\n",
    "\n",
    "        # Grab the batch of data and send it to the correct device\n",
    "        train_X, train_Y = next(train_dataiterator)\n",
    "        train_X, train_Y = train_X.to(device), train_Y.to(device)\n",
    "\n",
    "        # Compute the output\n",
    "        train_output = model(train_X)\n",
    "\n",
    "        # Compute loss\n",
    "        train_loss = criterion(train_output, train_Y)\n",
    "\n",
    "        num_in_batch = len(train_X)\n",
    "        tloss = train_loss.item() * num_in_batch / train_N\n",
    "        train_loss_mean += tloss\n",
    "        train_losses.append(train_loss.item())\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #\n",
    "    # Validation\n",
    "    #\n",
    "    model.eval()\n",
    "\n",
    "    valid_N = len(valid_loader.dataset)\n",
    "    num_valid_batches = len(valid_loader)\n",
    "\n",
    "    valid_loss_mean = 0\n",
    "    valid_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # valid_loader is probably just one large batch, so not using progress bar\n",
    "        for valid_X, valid_Y in valid_loader:\n",
    "\n",
    "            valid_X, valid_Y = valid_X.to(device), valid_Y.to(device)\n",
    "\n",
    "            valid_output = model(valid_X)\n",
    "\n",
    "            valid_loss = criterion(valid_output, valid_Y)\n",
    "\n",
    "            num_in_batch = len(valid_X)\n",
    "            vloss = valid_loss.item() * num_in_batch / valid_N\n",
    "            valid_loss_mean += vloss\n",
    "            valid_losses.append(valid_loss.item())\n",
    "\n",
    "            # Convert network output into predictions (one-hot -> number)\n",
    "            predictions = valid_output.argmax(1)\n",
    "\n",
    "            # Sum up total number that were correct\n",
    "            valid_correct += (predictions == valid_Y).type(torch.float).sum().item()\n",
    "\n",
    "    valid_accuracy = 100 * (valid_correct / valid_N)\n",
    "\n",
    "    # Report information\n",
    "    tloss = f\"Train Loss = {train_loss_mean:.4f}\"\n",
    "    vloss = f\"Valid Loss = {valid_loss_mean:.4f}\"\n",
    "    vaccu = f\"Valid Accuracy = {(valid_accuracy):>0.1f}%\"\n",
    "    mb.write(f\"[{epoch+1:>2}/{num_epochs}] {tloss}; {vloss}; {vaccu}\")\n",
    "\n",
    "    # Update plot data\n",
    "    max_loss = max(max(train_losses), max(valid_losses))\n",
    "    min_loss = min(min(train_losses), min(valid_losses))\n",
    "\n",
    "    x_margin = 0.2\n",
    "    x_bounds = [0 - x_margin, num_epochs + x_margin]\n",
    "\n",
    "    y_margin = 0.1\n",
    "    y_bounds = [min_loss - y_margin, max_loss + y_margin]\n",
    "\n",
    "    valid_Xaxis = torch.linspace(0, epoch + 1, len(train_losses))\n",
    "    valid_xaxis = torch.linspace(1, epoch + 1, len(valid_losses))\n",
    "    graph_data = [[valid_Xaxis, train_losses], [valid_xaxis, valid_losses]]\n",
    "\n",
    "    mb.update_graph(graph_data, x_bounds, y_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e54fdbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
